---
title: "Assignment"
description: |
  This assignment is aim to solve the problems of for Mini Challenge 2
author:
  - name: LI NAN
    url: https://www.linkedin.com/in/li-nan-63b9251a6/
date: 07-12-2021
output: 
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 1



---

## 1.Data Preparation

### 1.1 Global Settings

The global settings of R code chunks in this post is set as follows. 

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3,
                      echo = TRUE,
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

### 1.2 R Packages Installation

The following code input is to prepare for R Packages Installation.

```{r}
packages = c('raster','sf','tmap', 'clock','DT', 'ggiraph', 'plotly', 'tidyverse','dplyr','readr','hrbrthemes','tmap','mapview','shiny')
for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}

```

### 1.3 Data Import
The following code is to import raw data sets from [<font size="3"  color="blue">*Mini Challenge2*</font>](https://vast-challenge.github.io/2021/MC2.html)("*car-assignment.csv*","*cc_data.csv*","*gps.csv*","*loyalty_data.csv*").

```{r}
credit_debit <- read_csv("data/cc_data.csv")
loyalty_data <- read_csv("data/loyalty_data.csv")
car_assignment <- read_csv("data/car_assignments.csv")
GPS <- read_csv("data/gps.csv")

```
## 2.Tasks and Questions for Mini-Challenge2

### 2.1 Q1 Intruoduction

Using just the credit and loyalty card data, identify the most popular locations, and when they are popular. What anomalies do you see? What corrections would you recommend to correct these anomalies?


#### 2.1.1 Data Preparation for Q1


**Comparison of total amount between credit/debit card and loyalty card**



To know which place is most populated and when it is populated, we need a data table to list the most populated places and its time.

```{r}
loyalty_data$count_event=1
credit_debit$count_event=1

aggregate_dataset <- loyalty_data %>% 
    group_by(timestamp,location) %>% 
    dplyr::summarize(Frequency = sum(count_event),Money_loyalty=sum(price))


credit_debit$timestamp <- as.Date(credit_debit$timestamp, "%m/%d/%Y")
aggregate_cc <- credit_debit %>% 
    group_by(timestamp,location) %>% 
    dplyr::summarize(Frequency = sum(count_event),Money_cd=sum(price))



```

**Adjustment of Date Type and create a new column named "Day"**
```{r}
aggregate_dataset$timestamp <- as.Date(aggregate_dataset$timestamp, "%m/%d/%Y")

aggregate_dataset$Day <- format(aggregate_dataset$timestamp, format="%d")
aggregate_cc$Day <- format(aggregate_cc$timestamp, format="%d")
```

**Aggregation of loyalty cost and credit_debit cost**
```{r}
loyalty_money <- aggregate_dataset %>% group_by(Day,location) %>% dplyr::summarise(money_loyal=sum(Money_loyalty),freq_loyal=sum(Frequency))

cc_money <- aggregate_cc %>% group_by(Day,location) %>% dplyr::summarise(money_cc = sum(Money_cd),freq_cc=sum(Frequency))
```

**Table 2.1**Combination of loyalty_money and cc_money
```{r}


Comparison <- full_join(cc_money, loyalty_money, by = c('Day','location'))
Comparison[is.na(Comparison)] <- 0
Comparison$Money_dif=Comparison$money_cc-Comparison$money_loyal
Comparison$Freq_dif=Comparison$freq_cc-Comparison$freq_loyal

Comparison<-Comparison%>%
arrange(freq_cc)

datatable(Comparison,rownames = FALSE)


```
During data exploration, we can see there are five records that don't have any cost in credit card and debit card,but there are consumption records in loyalty card.

**Table 2.2**Combination of loyalty_money and cc_money
```{r}



Result1 <- Comparison  %>%
dplyr::group_by(Day) %>%
filter(freq_cc == max(freq_cc)) %>%
arrange(desc(Day))

datatable(Result1,rownames = FALSE)



```
From the new data frame "Result1", Now we can see that Katerina's Cafe is the most popular place based on data records from Day 6 to Day 19,which appears 6 times in 14 days records.

To find out more anomalies from the data, we need more obvious data visualization.

#### 2.1.2 Data Visualization

**new column: text for tooltip**

```{r}

Comparison$Money_dif <- round(Comparison$Money_dif ,2)

Comparison <- Comparison %>%
  mutate(text = paste0("Location: ", location, "\n", "Day of January: ", Day, "\n", "Money Difference: ",Money_dif))



Comparison <- Comparison%>%
  mutate(text2 = paste0("Location: ", location, "\n", "Day of January: ", Day, "\n", "Frequency Difference: ",Freq_dif))
```


**Heat map of money difference**

```{r,fig.height=8,fig.width=10}
p <- ggplot(data = Comparison, aes(x=Day, y=location,fill=Money_dif,text=text)) + 
  geom_tile() +
  geom_text(aes(label = Money_dif)) +
  scale_fill_gradient(low="pink", high="blue") +
  theme_ipsum()

p <- p + theme(axis.text.y = element_text(size = 8))

ggplotly(p, tooltip="text")
```




**Heat map of frequency difference**

```{r,fig.height=6,fig.width=10}
z <- ggplot(data = Comparison, aes(x=Day, y=location,fill=Freq_dif,text=text2)) + 
  geom_tile() +
  scale_fill_gradient(low="light yellow", high="red") +
  geom_text(aes(label = Freq_dif))+
  theme_ipsum()

z <- z + theme(axis.text.y = element_text(size = 8))

ggplotly(z, tooltip="text2")
```







#### 2.1.3 Infer and Analysis
we can see more anomalies comparing these two heat maps:
1.In these two weeks,except Maximum Iron and steel which the differences in money and frequencies are both 0, other places in these two weeks all appear difference in some days either in frequency or money.

2.In Frydos Auto Supply on Day 13, it has a large cost consumption of 9912.43 but the heat map of frequency difference shows 0 in frequency difference. It is quite strange.

3.Another anomaly is from data table 4.1, there are five records showing that credit card and debit card consumption cost are 0, but loyalty card has consumption records .And among these 5 records, the most doubtful part is that Stewart and Sons Fabrication in Day 13 has 4071.95 cost,which is also needed to be noted.




### 2.2 Q2 Intruoduction
Add the vehicle data to your analysis of the credit and loyalty card data. How does your assessment of the anomalies in question 1 change based on this new data? What discrepancies between vehicle, credit, and loyalty card data do you find? Please limit your answer to 8 images and 500 words.

#### 2.2.1 Data Preparation for Q2
To proceed in the Q2, we decide to have data manipulation for another two datasets GPS and car_Assignment

**Data Manipulation for car_Assignment data set(Make up full name)**

```{r}
car_assignment <-car_assignment %>% unite("Full Name", LastName:FirstName, remove = FALSE)
```

```{r}
car_assign2 = subset(car_assignment, select = -c(LastName,FirstName) )
head(car_assign2)
```

**Data Manipulation for GPS data set**

```{r}
glimpse(GPS)
```

First,"MC2-tourist.jpg" is imported for data preparation.

```{r}
bgmap <- raster("Data/MC2-tourist.tif")
bgmap
```



```{r}
tm_shape(bgmap) +
tm_rgb(bgmap, r = 1,g = 2,b = 3,
       alpha = NA,
       saturation = 1,
       interpolate = TRUE,
       max.value = 255)
```

```{r}
Abila_st <- st_read(dsn = "Data/Geospatial",
                    layer = "Abila")

```




```{r}
GPS$Timestamp <- strptime(GPS$Timestamp, "%m/%d/%Y %H:%M:%S")
GPS$day <- as.factor(get_day(GPS$Timestamp))

```

```{r}
GPS$id <- as_factor(GPS$id)
```

```{r}
glimpse(GPS)

```

### 这一步起应该与老师有所不同 ###



```{r}
GPS_sf <- st_as_sf(GPS, 
                   coords = c("long", "lat"),
                       crs= 4326)
GPS_sf
```

```{r}
gps_path <- GPS_sf %>%
  group_by(id,day) %>%
  summarize(m = mean(Timestamp), 
            do_union=FALSE) %>%
  st_cast("LINESTRING")
gps_path
```

```{r} 
p = npts(gps_path, by_feature = TRUE)
gps_path2 <- cbind(gps_path, p)
gps_path2 <- dplyr::filter(gps_path2,p!=1)
```






```{r}
head(gps_path2)
head(gps_path)
```

```{r}
gps_path_selected <- gps_path2 %>%
  group_by(id,day)%>%
  filter(day==13
         )
tmap_mode("view")
tm_shape(bgmap) + 
  tm_rgb(bgmap, r = 1,g = 2,b = 3,
       alpha = NA,
       saturation = 1,
       interpolate = TRUE,
       max.value = 255) +
  tm_shape(gps_path_selected) +
  tm_lines()
```

Q3 Can you infer the owners of each credit card and loyalty card? What is your evidence? Where are there uncertainties in your method? Where are there uncertainties in the data? Please limit your answer to 8 images and 500 words.

**Answer**: To answer Q3's question, we need to build a relationship between credit card & loyalty card owner and car owner. So this relationship connection is based on parking car site and location in the map. We need to find a logical time gap that can be the proof to help real location site to match parking car site.

So first of all, restructure GPS data set and build a box plot to find the appropriate time gap for parking car.

```{r}
GPS_sf <- GPS_sf %>%
  dplyr::arrange(day, Timestamp) %>%
  group_by(id,day) %>%
  mutate(diff = Timestamp - lag(Timestamp),
         diff_mins = as.numeric(diff, units = 'mins'))


```

```{r}
GPS_sf$diff_mins <- round(GPS_sf$diff_mins ,2)
GPS_sf <- GPS_sf %>%
  mutate_at(vars(diff_mins), ~replace_na(., 0))

```


Now we want to justify which time gap is suitable for a parking time period, we build up a box plot based on dif_mins column.

```{r}
boxplot1=ggplot(GPS_sf,aes(x="",y=GPS_sf$diff_mins))+geom_boxplot()+labs(title="Distribution of time gap")+theme_classic()


ggplotly(boxplot1)

```
Since the boxplot shows that more than 3/4 data points of dif_mins column are 0, it is useless for us to have a justification for parking time. So we can only assume that data points whose dif_mins >3 are recognized as parking point. Based on this assumption, we build up a new column called parking point.

```{r}
GPS_sf$park_point <- 0
GPS_sf$park_point[GPS_sf$diff_mins >3 ] <- 1
GPS_sf$park_point[GPS_sf$diff_mins <=3 ] <- 0



```





```{r}
Tracker_GPS <- GPS_sf %>% group_by(day,id) %>% dplyr::summarise(Daily_Park=sum(park_point))

```